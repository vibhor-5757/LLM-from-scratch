{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOuTjLloVXE291TjA2FbqF2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1D78h6KYemE"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"lMtBZihFYyNH"}},{"cell_type":"code","source":["import os\n","import random\n","\n","import torch"],"metadata":{"id":"B9H6k06GYxdq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Self attention\n","\n","we split the input embeddings of tokens (positional + token) into 3 matrices, key query and value, with trainable weights.\n","\n","the dimensions of the weight matrixes are (embedding_dim, out_dim).\n","\n","when the input with dimension (context_length, embedding_dim) is multiplied with the weight matrixes, the final query,key and value matrixes have dim (context_length, out_dim)\n","\n","the out_dim can be any value we want, and this can be can be used to change the dimension of the inputs. (gpt keeps same dimensions)\n","\n","we multiply the key and query.Transpose. matrixes. the result of this is attention score matrix with dimensions (context_length, context_length). the cell [2,3] in this matrix will give us the attention score between token 2 and token 3. ie. how much attention should token 3 be given when we are seeing token 2.\n","\n","This attention score matrix is normalized by applying dividing by the sqrt of out_dim. this makes the varience as 1 and also results in less peaky results after softmax is applied. this is because it reduces the magnitude of each value. softmax function is applied to the values in the matrix to convert the values into percentages.\n","\n","after normalization the attention values matrix is converted into attention weights matrix.\n","\n","the attention weights matrix is multiplied with values matrix to get the final context vectors. the dimension of the context vector matrix is (context_length, out_dim)"],"metadata":{"id":"dsK4U9KWY1Sp"}},{"cell_type":"code","source":["inputs = torch.tensor(\n","  [[0.43, 0.15, 0.89], # Your     (x^1)\n","   [0.55, 0.87, 0.66], # journey  (x^2)\n","   [0.57, 0.85, 0.64], # starts   (x^3)\n","   [0.22, 0.58, 0.33], # with     (x^4)\n","   [0.77, 0.25, 0.10], # one      (x^5)\n","   [0.05, 0.80, 0.55]] # step     (x^6)\n",")"],"metadata":{"id":"65uv3cqhY32Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06NN8wVhd_Zi","executionInfo":{"status":"ok","timestamp":1741180896936,"user_tz":-330,"elapsed":8,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"2106b810-5d72-49a5-b944-1a7e4e8451d1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([6, 3])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["context_length = inputs.shape[0]"],"metadata":{"id":"vi4ySrQ4h5ul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d_in = inputs.shape[1]\n","d_out = 2\n","qkv_bias = False"],"metadata":{"id":"KXppz-MJc0Mt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n","W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n","W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)"],"metadata":{"id":"fjulRatgcT9g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["this is a 2 * 3 matrix and not a 3 * 2 matrix cause pytorch does the calculation in its linear layer as X * W.T, and hence stores the W.T as its weights, not the W directly"],"metadata":{"id":"JQ0PDxUge7tk"}},{"cell_type":"code","source":["W_query.weight"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jYuiMS6eLww","executionInfo":{"status":"ok","timestamp":1741180967875,"user_tz":-330,"elapsed":32,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"2b8bd88b-9e55-470a-c412-18365d1a823d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.1327, -0.4735,  0.3015],\n","        [ 0.3907,  0.5241,  0.2294]], requires_grad=True)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["queries = W_query(inputs)\n","keys = W_key(inputs)\n","values = W_value(inputs)"],"metadata":{"id":"PjaY9ZnMc_k3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"keys.shape:\", keys.shape)\n","print(\"values.shape:\", values.shape)\n","print(\"queries.shape:\", queries.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nM_7r-BTffg7","executionInfo":{"status":"ok","timestamp":1741181310042,"user_tz":-330,"elapsed":18,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"ab82c0f5-7471-411c-c3f3-a3a3308d0683"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["keys.shape: torch.Size([6, 2])\n","values.shape: torch.Size([6, 2])\n","queries.shape: torch.Size([6, 2])\n"]}]},{"cell_type":"code","source":["attn_scores = queries @ keys.T\n","attn_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYqHkLSifcWc","executionInfo":{"status":"ok","timestamp":1741181369314,"user_tz":-330,"elapsed":15,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"bb38da75-c7d1-46b1-ba5b-f878feb38932"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3549, 0.4275, 0.4244, 0.2210, 0.2488, 0.2676],\n","        [0.5914, 0.7866, 0.7795, 0.4202, 0.4333, 0.5177],\n","        [0.5860, 0.7796, 0.7727, 0.4165, 0.4294, 0.5132],\n","        [0.3306, 0.4460, 0.4419, 0.2393, 0.2437, 0.2955],\n","        [0.3238, 0.4355, 0.4315, 0.2334, 0.2384, 0.2881],\n","        [0.4040, 0.5408, 0.5359, 0.2894, 0.2968, 0.3570]],\n","       grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["attn_scores.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_ed20aMfohE","executionInfo":{"status":"ok","timestamp":1741181371281,"user_tz":-330,"elapsed":5,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"0d399c36-603d-4593-a58e-914776b60e47"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([6, 6])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["d_out = keys.shape[-1]\n","attn_weights = torch.softmax(attn_scores / d_out**0.5, dim=-1)\n","\n","print(attn_weights)\n","print(d_out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0rFxQm8fvJT","executionInfo":{"status":"ok","timestamp":1741181398112,"user_tz":-330,"elapsed":18,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"cbc70e50-80c3-4859-e6fe-4177cded4ffd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1700, 0.1790, 0.1786, 0.1547, 0.1578, 0.1599],\n","        [0.1661, 0.1907, 0.1898, 0.1472, 0.1485, 0.1577],\n","        [0.1661, 0.1905, 0.1896, 0.1474, 0.1487, 0.1578],\n","        [0.1661, 0.1802, 0.1797, 0.1557, 0.1562, 0.1620],\n","        [0.1662, 0.1799, 0.1794, 0.1559, 0.1565, 0.1621],\n","        [0.1662, 0.1831, 0.1825, 0.1533, 0.1541, 0.1608]],\n","       grad_fn=<SoftmaxBackward0>)\n","2\n"]}]},{"cell_type":"code","source":["context_vectors = attn_weights @ values\n","context_vectors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugs-iAVeg_UX","executionInfo":{"status":"ok","timestamp":1741181688506,"user_tz":-330,"elapsed":27,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"788e675d-63d0-4b19-884f-1a52a3cb7c63"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0774, -0.0545],\n","        [-0.0763, -0.0572],\n","        [-0.0762, -0.0572],\n","        [-0.0752, -0.0567],\n","        [-0.0753, -0.0566],\n","        [-0.0756, -0.0567]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["context_vectors.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hK8RhnafhCKP","executionInfo":{"status":"ok","timestamp":1741181696384,"user_tz":-330,"elapsed":18,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"0007a4e8-e508-4dea-ec1c-6e06308a60ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([6, 2])"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["# Causal attention\n","\n","the concept in causal attention is that should only consider the current and previous tokens to calculate the attention weights. the future values should be hidden and hence thier attention weighhts should be 0\n","\n","we can implement this by substituting all future values in the attention scores matrix with -inf. hence when the softmax is applied, those values get converted to 0.\n","\n","along with this, a dropout layer is also added. this stops a neuron from getting lazy and start depending on other neurons for predictions.\n","\n","dropout layer can be implemented after the attention weights are calculated (more common) or after the context_vector is created.\n","\n","in pytorch implementation of dropout layer, the values which are not dropped are scaled by a factor of 1/1-p. ie. if dropout is implemented with value 0.5, the rest of the values will be multiplied by value 2"],"metadata":{"id":"MsBAN6BjgAiV"}},{"cell_type":"code","source":["mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n","mask"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"enNtkpG-h31E","executionInfo":{"status":"ok","timestamp":1741181945422,"user_tz":-330,"elapsed":32,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"39469d15-2e50-4b0f-ee53-01d5580f3b50"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 1., 1., 1., 1., 1.],\n","        [0., 0., 1., 1., 1., 1.],\n","        [0., 0., 0., 1., 1., 1.],\n","        [0., 0., 0., 0., 1., 1.],\n","        [0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["causal_attn_scores = attn_scores.masked_fill(mask.bool(), -torch.inf)\n","print(causal_attn_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t5bnxS36gDN9","executionInfo":{"status":"ok","timestamp":1741182123646,"user_tz":-330,"elapsed":26,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"3b122c58-866c-466c-f87c-0a6ee60db3e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.3549,   -inf,   -inf,   -inf,   -inf,   -inf],\n","        [0.5914, 0.7866,   -inf,   -inf,   -inf,   -inf],\n","        [0.5860, 0.7796, 0.7727,   -inf,   -inf,   -inf],\n","        [0.3306, 0.4460, 0.4419, 0.2393,   -inf,   -inf],\n","        [0.3238, 0.4355, 0.4315, 0.2334, 0.2384,   -inf],\n","        [0.4040, 0.5408, 0.5359, 0.2894, 0.2968, 0.3570]],\n","       grad_fn=<MaskedFillBackward0>)\n"]}]},{"cell_type":"code","source":["causal_attn_weights = torch.softmax(causal_attn_scores / d_out**0.5, dim=-1)\n","causal_attn_weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8l_C6QciKNq","executionInfo":{"status":"ok","timestamp":1741182125393,"user_tz":-330,"elapsed":42,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"8d70e237-b205-4d34-9895-339da34ae75e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.4656, 0.5344, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3042, 0.3488, 0.3471, 0.0000, 0.0000, 0.0000],\n","        [0.2436, 0.2644, 0.2636, 0.2284, 0.0000, 0.0000],\n","        [0.1984, 0.2147, 0.2141, 0.1861, 0.1868, 0.0000],\n","        [0.1662, 0.1831, 0.1825, 0.1533, 0.1541, 0.1608]],\n","       grad_fn=<SoftmaxBackward0>)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["causal_attn_weights = torch.nn.Dropout(0.5)(attn_weights)\n","causal_attn_weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a24hacmPj1dn","executionInfo":{"status":"ok","timestamp":1741182480791,"user_tz":-330,"elapsed":35,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"d6105eb3-04d8-49f9-9075-30fd18e9a773"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0000, 1.0689, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.6976, 0.6941, 0.0000, 0.0000, 0.0000],\n","        [0.4873, 0.5287, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3968, 0.0000, 0.0000, 0.3722, 0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.3649, 0.3066, 0.0000, 0.3216]],\n","       grad_fn=<MulBackward0>)"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["causal_context_vectors = attn_weights @ values\n","causal_context_vectors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-_Sj3Soib14","executionInfo":{"status":"ok","timestamp":1741182490013,"user_tz":-330,"elapsed":27,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"e1f0e1d0-96a5-4401-c704-df690906bec8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.4708,  0.3378],\n","        [-0.2341,  0.0793],\n","        [-0.1632,  0.0026],\n","        [-0.1154, -0.0295],\n","        [-0.1049, -0.0297],\n","        [-0.0756, -0.0567]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["# Multi head attention\n","\n","we repeat the task we did in casual attention multiple times. finally we concantenate the context vectors along the columns to get the final output.\n","\n","eg if we run the above causal attn mech 12 times or we create 12 heads, we will get 12 matrixes of dim (6,2). upon thier concatenation, the final output will be of dimension (6, 2*12) or (6, 24)\n","\n","running the causal attn mech multiple times is computationally intensive, as we are doing matrix multiplication multiple times. a better way to do this is to combine the different keys, values and queries matrixes into 1 big matrix for each.\n","\n","steps:\n","- 1. take input with dimension (batch, context_length, embedding_dimension)\n","\n","- 2. decide how many heads to use, and upon the final output dimension after concatenation you want to get. get dimension of each head, ie the output dimension of each head by dividing the final output dimension with the number of heads.\n","\n","head_dim = final_out_dim / num_heads\n","\n","- 3. make the trainable key query and value weight matrices with dimensions (embedding_dim, final_out_dim)\n","\n","- 4. make the key, value and query matrixes by multiplying the input and the weight matrixes. the dimensions of these will be (batch, context_length, final_out_dim)\n","\n","- 5. split the final_out_dim into 2 dimensions: num_heads and head_dim. ie the dimension now become: (batch, context_length, num_heads, head_dim)\n","\n","- 6. group by num_heads instead of context_length. ie dim now become: (batch, num_heads, context_length, head_dim)\n","\n","- 7. find out the attn_scores by multiplying queries and key.Transpose(2,3). ie. mul (batch, num_heads, context_length, head_dim) * (batch, num_heads, head_dim, context_length), to get dim:(batch, num_heads, context_length, context_length)\n","\n","- 8. convert this to attention weights by normalizing (divide by sqrt of head dim) and masking future token scores. (mask with -inf and then taking softmax) also add dropout layer\n","\n","- 9.  find the context_vector by multiplying attn_weights and values.\n","ie. (batch, num_heads, context_length, context_length) * (batch, num_heads, context_length, head_dim) to get (batch, num_heads, context_length, head_dim)\n","\n","- 10. group num_heads and head_dim again to get final_out_dim. ie (batch, num_heads, context_length, head_dim) --> (batch, context_length, num_heads, head_dim) --> (batch, context_length, final_out_dim)"],"metadata":{"id":"MfIjA6EenKr0"}},{"cell_type":"code","source":["class MultiHeadAttention(torch.nn.Module):\n","    def __init__(self, embedding_dim, context_length, num_heads,final_out_dim, dropout, qkv_bias=False):\n","        super().__init__()\n","        assert (final_out_dim % num_heads == 0), \\\n","            \"final_out_dim must be divisible by num_heads\"\n","\n","        self.final_out_dim =final_out_dim\n","        self.num_heads = num_heads\n","        self.head_dim =final_out_dim // num_heads # Reduce the projection dim to match desired output dim\n","\n","        self.W_query = torch.nn.Linear(embedding_dim,final_out_dim, bias=qkv_bias)\n","        self.W_key = torch.nn.Linear(embedding_dim,final_out_dim, bias=qkv_bias)\n","        self.W_value = torch.nn.Linear(embedding_dim,final_out_dim, bias=qkv_bias)\n","\n","        self.out_proj = torch.nn.Linear(final_out_dim,final_out_dim)  # Linear layer to combine head outputs\n","\n","        self.dropout = torch.nn.Dropout(dropout)\n","\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, embedding_dim = x.shape\n","\n","        keys = self.W_key(x) # Shape: (b, num_tokens,final_out_dim)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # We implicitly split the matrix by adding a `num_heads` dimension\n","        # Unroll last dim: (b, num_tokens,final_out_dim) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n","        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n","\n","        # Original mask truncated to the number of tokens and converted to boolean\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Use the mask to fill attention scores\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        # attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n","\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Shape: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.final_out_dim)\n","\n","        context_vec = self.out_proj(context_vec) # optional projection\n","\n","        return context_vec"],"metadata":{"id":"wd-D6HqwoG9S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = torch.tensor(\n","    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n","     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n","     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",")"],"metadata":{"id":"WtlrWr5kvLxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch = torch.stack((inputs, inputs), dim=0)\n","batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLkUfkAsvPIK","executionInfo":{"status":"ok","timestamp":1741185450144,"user_tz":-330,"elapsed":23,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"45b1eb54-b6dd-4b10-9817-22dc4d721b9f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]],\n","\n","        [[0.4300, 0.1500, 0.8900, 0.5500, 0.8700, 0.6600],\n","         [0.5700, 0.8500, 0.6400, 0.2200, 0.5800, 0.3300],\n","         [0.7700, 0.2500, 0.1000, 0.0500, 0.8000, 0.5500]]])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["print(batch.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kJGG5d-vUw9","executionInfo":{"status":"ok","timestamp":1741185452946,"user_tz":-330,"elapsed":23,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"d49c6e90-64bf-4cd0-951b-77a1f5111ea6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 6])\n"]}]},{"cell_type":"code","source":["batch_size, context_length, embedding_dim = batch.shape\n","\n","final_out_dim = 10\n","\n","mha = MultiHeadAttention(embedding_dim = embedding_dim, context_length = context_length,  num_heads=2, final_out_dim = final_out_dim, dropout = 0.1)\n","context_vecs = mha(batch)\n","context_vecs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLgqthRvvSLs","executionInfo":{"status":"ok","timestamp":1741185915732,"user_tz":-330,"elapsed":62,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"713054b4-b1c2-427a-a9d8-4613464250b7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.0046,  0.1308,  0.5578, -0.0186, -0.1088, -0.3180,  0.0801,\n","          -0.3551,  0.0438, -0.2381],\n","         [-0.0131,  0.0761,  0.5948,  0.0289, -0.1445, -0.2143,  0.1580,\n","          -0.2767,  0.0612, -0.1447],\n","         [ 0.0160, -0.0773,  0.6198,  0.1682, -0.1984, -0.0387,  0.2471,\n","          -0.1435,  0.1482, -0.0367]],\n","\n","        [[ 0.0046,  0.1308,  0.5578, -0.0186, -0.1088, -0.3180,  0.0801,\n","          -0.3551,  0.0438, -0.2381],\n","         [-0.0131,  0.0761,  0.5948,  0.0289, -0.1445, -0.2143,  0.1580,\n","          -0.2767,  0.0612, -0.1447],\n","         [-0.0471,  0.1035,  0.5789,  0.0709, -0.1041, -0.1932,  0.1237,\n","          -0.2931,  0.0085, -0.1576]]], grad_fn=<ViewBackward0>)"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["print(\"context_vecs.shape: \", context_vecs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0FWcR8-vsfe","executionInfo":{"status":"ok","timestamp":1741185917860,"user_tz":-330,"elapsed":10,"user":{"displayName":"wang hao","userId":"12229083387668301812"}},"outputId":"5ce3ec3b-8ccd-4d48-f878-3681179561af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["context_vecs.shape:  torch.Size([2, 3, 10])\n"]}]}]}